{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from huggingface_hub import login\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870dc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.environ.get(\"Hf_token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26966cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "max_sequence_length = 1024\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_sequence_length,\n",
    "    dtype = dtype,\n",
    "    gpu_memory_utilization = 0.88,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    device_map = \"cuda\",\n",
    "    token = hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"\n",
    "Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\n",
    "\n",
    "Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\n",
    "\n",
    "### Task:\n",
    "You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\n",
    "\n",
    "### Query:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or\n",
    "              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
    "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate (\n",
    "    input_ids = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask,\n",
    "    max_new_tokens = 1200,\n",
    "    use_cache = True\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0].split(\"### Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd75aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[:500]\", trust_remote_code = True)\n",
    "     \n",
    "\n",
    "medical_dataset[1]\n",
    "     \n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  \n",
    "EOS_TOKEN\n",
    "\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
    "Please answer the following medical question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\n",
    "{}\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9151a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_data(examples):\n",
    "  inputs = examples[\"Question\"]\n",
    "  cots = examples[\"Complex_CoT\"]\n",
    "  outputs = examples[\"Response\"]\n",
    "\n",
    "  texts = []\n",
    "\n",
    "  for input, cot, output in zip(inputs, cots, outputs):\n",
    "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "    texts.append(text)\n",
    "\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset = medical_dataset.map(preprocess_input_data, batched = True)\n",
    "finetune_dataset[\"texts\"][0]\n",
    "model_lora = FastLanguageModel.get_peft_model(\n",
    "    model = model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3047,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fdceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# --- 5. TRAIN ---\n",
    "print(\"Initializing Trainer (Safe Mode)...\")\n",
    "\n",
    "model.config.use_cache = False \n",
    "\n",
    "# !!! DEBUG STEP: Try with just 500 samples first !!!\n",
    "# If this works, your full dataset is simply too big for your RAM.\n",
    "# You can remove .select(range(500)) later if you upgrade your RAM.\n",
    "train_subset = medical_dataset.select(range(500)) \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_subset, # Using the subset\n",
    "    dataset_text_field = \"text\",\n",
    "    \n",
    "    # REDUCED CONTEXT: Higher values (4096+) kill RAM\n",
    "    max_seq_length = 2048, \n",
    "    \n",
    "    # CRITICAL MEMORY FIX: Disable packing\n",
    "    # Packing = True combines multiple short examples into one.\n",
    "    # It is faster but uses 2x-3x more RAM. We disable it here.\n",
    "    packing = False, \n",
    "    \n",
    "    dataset_num_proc = 1,\n",
    "    dataset_kwargs = {\n",
    "        \"keep_in_memory\": False,\n",
    "        \"num_proc\": 1 \n",
    "    },\n",
    "    \n",
    "    formatting_func = None, \n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        dataloader_num_workers = 0, \n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "\n",
    "\n",
    "# Step10: Testing after fine-tuning\n",
    "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n",
    "              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
    "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model_lora)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model_lora.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81788c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0].split(\"### Answer:\")[1])\n",
    "\n",
    "\n",
    "question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,\n",
    "              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,\n",
    "              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.\n",
    "              What is the most likely predisposing factor for this patient's condition?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model_lora)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model_lora.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Decode the response tokens back to text\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response[0].split(\"### Answer:\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
